##############################################################################
#                                                                            #
#    Benjamin Fildier, Mar 2017.                                             #
#                                                                            #
#    This script extract, from SPCAM hourly outputs, values for updraft and  #
#    downdraft velocities CRM_WUP_IXX and CRM_WDN_IXX, for a given           #
#    experiment name, input directory and time boundaries. It reads the      #
#    CRM points of XX% largest precipitation rates and computes the mean of  #
#    V over these points within each GCM cell.                               #
#                                                                            #
#    Prerequisites : outputdir/I${fraction} file previously generated by     #
#                        script getDailyCRMIndAbovePrecFraction.py           #
#                    varid has to have vertical dimension                    #
#    Arguments     : varid fraction experiment case datetime1 datetime2      #
#                        inputdir                                            #
#    Output file   : ${varid}-I${fraction}_1hr_CESM111-SPCAM20_...           #
#                     ...${experiment}_r1i1p1_${datetime1}-${datetime2}.nc   #
#                                                                            #
##############################################################################


###--- Modules ---###
import glob
import sys,os
from netCDF4 import Dataset
from datetime import datetime
import numpy as np
import warnings
import socket

currentpath = os.path.dirname(os.path.realpath(__file__))
sys.path.insert(0,currentpath[:currentpath.rfind('/')+1]+'functions')

###--- Functions ---###

## Check we get correct arguments and return the tuple:
## (fraction, experiment, case, datetime1, datetime2, list of inputdir/*.nc files)
def parseArguments():
	args = sys.argv[1:]
	# Check number of arguments
	if len(args) != 6:
		print "Wrong number of arguments.\nRequired arguments: fraction "+\
		"experiment case datetime1 datetime2 inputdir"
		sys.exit(1)
	# Get list of files in directory (3rd argument)
	fraction, experiment, case, datetime1, datetime2 = args[:5]
	# Get time boundaries
	dt_bnds = []
	for tstamp in [datetime1,datetime2]:
		dt_info = [int(s) for s in tstamp.split('-') if s.isdigit()]
		dt_bnds.append(datetime(dt_info[0],dt_info[1],dt_info[2],dt_info[3]/3600))
	# Change date format to CMIP5 standard
	datetime1 = dt_bnds[0].isoformat().replace('-','').replace('T','').replace(':','')[:-2]
	datetime2 = dt_bnds[1].isoformat().replace('-','').replace('T','').replace(':','')[:-2]
	# Store correct files (between time boundaries, with correct case name)
	ncfiles = []
	for file in glob.glob(args[-1]+'/*.nc'):
		filename = os.path.basename(file)
		dt_info = [int(s) for s in filename.split('.')[3].split('-') if s.isdigit()]
		dt = datetime(dt_info[0],dt_info[1],dt_info[2],dt_info[3]/3600)
		if case in filename and ".cam.h0." in filename:
			if dt <= dt_bnds[1] and dt >= dt_bnds[0]:
				ncfiles.append(file)
	# Exit if no interesting files are found
	if len(ncfiles) == 0:
		print "No files matching request."
		sys.exit(0)
	ncfiles.sort()
	return int(fraction), experiment, case, datetime1, datetime2, ncfiles


###--- Main program ---###

if __name__ == '__main__':

##-- Get arguments --##

	fraction, experiment, case, datetime1, datetime2, ncfiles = parseArguments()
	varid = "CRM_W"
	newvaridup = varid+"UP_I%d"%fraction
	newvariddown = varid+"DN_I%d"%fraction
	indvarid = "I%d"%fraction

##-- Set directories --##

	#- Input -#
	inputdir = os.path.dirname(ncfiles[0])

	#- Output -#
	hostname = socket.gethostname()
	if hostname == "jollyjumper":
		outputdir = os.path.join(os.path.dirname(os.path.dirname(inputdir)),
			'preprocessed',case,'day')
	elif "edison" in hostname:
		outputdir = os.path.join(os.path.dirname(currentpath),'preprocessed',case,'day')
	outputfile_up = newvaridup.replace('_','-')+"_day_CESM111-SPCAM20_"+experiment+\
	"_r1i1p1_"+datetime1[:8]+'-'+datetime2[:8]+".nc"
	outputfile_down = newvariddown.replace('_','-')+"_day_CESM111-SPCAM20_"+experiment+\
	"_r1i1p1_"+datetime1[:8]+'-'+datetime2[:8]+".nc"

	#- Input -#
	inddir = os.path.join(os.path.dirname(outputdir),'1hr')
	indfile = indvarid.replace('_','-')+"_1hr_CESM111-SPCAM20_"+experiment+\
	"_r1i1p1_"+datetime1+'-'+datetime2+".nc"

##-- Create output files --##

	fh_source = Dataset(ncfiles[0],'r')
	lon_source = fh_source.variables['lon']
	lat_source = fh_source.variables['lat']
	vardims = fh_source.variables[varid].dimensions
	varshape = fh_source.variables[varid].shape
	nlev = varshape[1]

	#- Create files -#
	rootgrp_up = Dataset(os.path.join(outputdir,outputfile_up),'w')
	rootgrp_down = Dataset(os.path.join(outputdir,outputfile_down),'w')

	#- Create dimensions and global attributes -#
	nlon = len(lon_source)
	rootgrp_up.createDimension("lon",nlon)
	rootgrp_down.createDimension("lon",nlon)
	nlat = len(lat_source)
	lat_slice = slice(nlat/3,2*nlat/3)
	rootgrp_up.createDimension("lat",nlat/3)
	rootgrp_down.createDimension("lat",nlat/3)
	rootgrp_up.createDimension("lev",nlev)
	rootgrp_down.createDimension("lev",nlev)
	rootgrp_up.createDimension("time",None)
	rootgrp_down.createDimension("time",None)
	rootgrp_up.case = str(fh_source.case)
	rootgrp_down.case = str(fh_source.case)
	rootgrp_up.description = "CRM-level updraft speed averaged over points contributing to the"+\
	" most intense "+str(fraction)+"%% CRM_PREC values in the CESM111-SPCAM20 "+\
	experiment+" AMIP experiment."
	rootgrp_down.description = "CRM-level downdraft speed averaged over points contributing to the"+\
	" most intense "+str(fraction)+"%% CRM_PREC values in the CESM111-SPCAM20 "+\
	experiment+" AMIP experiment."

	#- Define variables -#
	lon_up = rootgrp_up.createVariable("lon","f8",("lon",))
	lon_up[:] = lon_source[:]
	lon_down = rootgrp_down.createVariable("lon","f8",("lon",))
	lon_down[:] = lon_source[:]
	lat_up = rootgrp_up.createVariable("lat","f8",("lat",))
	lat_up[:] = lat_source[lat_slice]
	lat_down = rootgrp_down.createVariable("lat","f8",("lat",))
	lat_down[:] = lat_source[lat_slice]
	time_up = rootgrp_up.createVariable("time","f8",("time",))
	date_up = rootgrp_up.createVariable("date","i4",("time",))
	datesec_up = rootgrp_up.createVariable("datesec","i4",("time",))
	newvar_up = rootgrp_up.createVariable(newvaridup,"f4",("time","lev","lat","lon"))
	time_down = rootgrp_down.createVariable("time","f8",("time",))
	date_down = rootgrp_down.createVariable("date","i4",("time",))
	datesec_down = rootgrp_down.createVariable("datesec","i4",("time",))
	newvar_down = rootgrp_down.createVariable(newvariddown,"f4",("time","lev","lat","lon"))
	
	#- Define variable attributes -#
	lon_up.long_name = lon_down.long_name = str(lon_source.long_name)
	lat_up.long_name = lat_down.long_name = str(lat_source.long_name)
	date_up.long_name = date_down.long_name = str(fh_source.variables['date'].long_name)
	datesec_up.long_name = datesec_down.long_name = str(fh_source.variables['datesec'].long_name)
	lon_up.units = lon_down.units = str(lon_source.units)
	lat_up.units = lat_down.units = str(lat_source.units)
	time_up.units = time_down.units = str(fh_source.variables['time'].units)
	time_up.calendar = time_down.calendar = str(fh_source.variables['time'].calendar)
	newvar_up.long_name = newvar_down.long_name = str(fh_source.variables[varid].long_name)
	newvar_up.units = newvar_down.units = str(fh_source.variables[varid].units)

	fh_source.close()

##-- Read all files --##

	n = len(ncfiles)
	time_values = np.zeros((n,))
	date_values = np.zeros((n,))
	datesec_values = np.zeros((n,))
	var_values = np.zeros((n,varshape[1],varshape[2],varshape[3],nlat/3,nlon))

	# Start chronometer
	t0 = datetime.now()

	# Read in all varid values
	for i in range(n):

		ncfile = ncfiles[i]
		print "... Opening "+ncfile.split('.')[3]+" data ..."
		# Open file
		fh = Dataset(ncfile)
		# Copy values
		time_values[i] = fh.variables['time'][:]
		date_values[i] = fh.variables['date'][:]
		datesec_values[i] = fh.variables['datesec'][:]
		var_values[i] = fh.variables[varid][:,:,:,:,lat_slice,:]
		# Close file
		fh.close()

	# Open indvarid file
	fh_ind = Dataset(os.path.join(inddir,indfile),'r')
	ind_values = np.array(fh_ind.variables[indvarid][:],dtype=bool)

	# Initialize
	newvar_values_up = np.zeros((1,nlev,nlat/3,nlon))
	newvar_values_down = np.zeros((1,nlev,nlat/3,nlon))
	
	# Compute values
	for ilat in range(nlat/3):
		for ilon in range(nlon):
			# Stack filter indices over nlevels, level dimension staying first
			ind = ind_values[:,:,:,ilat,ilon]
			ind4D = np.vstack([[ind]]*nlev)
			# Swap time and lev dimension in the data
			values = var_values[:,:,:,:,ilat,ilon].swapaxes(0,1)
			# Apply filter-indices to the values but retain vertical dimension
			values_ok = values[ind4D].reshape(nlev,ind4D.sum()/nlev)
			values_ok_up = values_ok.copy()
			values_ok_up[values_ok < 0] = np.nan
			values_ok_down = values_ok.copy()
			values_ok_down[values_ok > 0] = np.nan
			with warnings.catch_warnings():
				warnings.simplefilter("ignore", category=RuntimeWarning)
				newvar_values_up[0,:,ilat,ilon] = np.nanmean(values_ok_up,axis=1)
				newvar_values_down[0,:,ilat,ilon] = np.nanmean(values_ok_down,axis=1)

	# Stop chronometer
	t1 = datetime.now()

	print
	print "------ It took", (t1-t0), "to process %d files."%n

##-- Write to output --##

	time_up[:] = time_down[:] = time_values[:].mean()
	date_up[:] = date_down[:] = date_values[0]
	datesec_up[:] = datesec_down[:] = datesec_values[:].mean()
	newvar_up[:] = newvar_values_up[:]
	newvar_down[:] = newvar_values_down[:]

	rootgrp_up.close()
	rootgrp_down.close()

	sys.exit(0)







